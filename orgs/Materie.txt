https://huggingface.co/docs/transformers/en/model_doc/bert


seaborn.heatmap(cosine_similarity(news_embeddings))

matplotlib: 
scatter - Used with points | grafic de tip Diagramă cu puncte (Scatter Chart)
hist()
plot(label="Sin")
bar
imshow - generates heatmaps and other things like that (hot, seismic)
legend
figure - Initializes a figure
title
show
savefig
xticks (how dense are numbers on the x-axis, rotate=45)
https://stackoverflow.com/questions/43027980/purpose-of-matplotlib-inline






numpy: 
array
linespace(num=how many points to interpolate)
arange(step=0.5) - returns evenly spaced numbers.
mean - facem media aritmetica
max
minimum(arr, 3.5) - broadcasting - change size of array to match another one for an operation,
mask = np_array>value => all indexes where the value in the array is higher than value
pm10_np[bmask]

np.reshape -> reshape array from let say (1, 100) to (10, 10)
np.stack -> takes 10 array of size 10 and stacks them on top of the other -> (10, 10) (i can also stack array generated by reshape)
arr.flatten() collapse to one dimension










DEX:
RL: Reinforced Learning
Out(live, liar)? Points
ReLU - Rectified Linear Unit (in deep learning un neuron nu da output daca valoarea pe care ar urma sa o da e sub un threshold, de ex -3<0 => 0)
Gradient = Derivata
Learning Rate = Cat de repede invata programul
torch.nn.Module; nn = Neural Networks
batch = Chunkuri mici de date folosite in antrenarea in PyTorch










Python:
arr[7:] all the elements after the 7th one
arr[:7] first 7 elements
arr[::2] first, third, ... element
arr[2::] works like arr[2:]
arr[..., :3]=arr[:, :, :3] ---> ...=all remaining dimensions
arr[:, 2], all lines, column 2
arr[:7, 2] first 7 lines, column 2
arr[::2, 2] every second line, column 2
(zi, locatie) -> (zi, locatie, indicator)
print ("%s, %d, %.2f....")
class Model(torch.nn.Module) ---> Mosteneste clasa modul din torch








Pillow:
im.size - coloane, linii
im.mode - RGB
im.format - PNG

pixel = picture elements
im_gray = np.mean(im, axis=-1)
plt.imshow(im_gray, cmap='gray')









Pandas:
import pandas as pd
df[..].loc(df[..]>500) ---> filtrare
df[..].uniques() ---> list unique values
df[..].value_counts() ---> count them
df.loc[condition, 'col'] ---> only those elements who have the condition.
df.drop(column='...', inplace=True) ---> remove column
df[..]=df[..].map({old: new})  ----> ex: map string to nr
df =  pd.get_dummies(df, columns=['...']) ---> map column to more columns of boolean value
df.info() ---> find cool data about table
df['...'].sum().min().max().mean()
df['...].loc(df['...'].isna)=df['...].mean() # To resolve not available data
X['...'].mode()[0] ---> most common value??





Clustering: K-Means algorithm
Clasificare: KNN - K-Nearest Neighbor Algorithm




datasets:
data=load_dataset("imdb")
td_data=data['train'/'test'].data_to_pydict()




sklearn: confusion_matrix
K-Nearest Neighbor Algorithm
Random Forest
train_test_split
Nitro NPL: Introduction to Machine Learning, ora 1:08
model.score(X, y)
https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html
sc=StandardScaler()
X_train=sc.fit_transform(X_train)
X_test=sc.transform(X_test)




PyTorch:
Straturi: Linear, Fully Connected, Dense = sinonime
Gradient Descent = w <- w - gradient*(learning rate)
Optimizer - cel care se ocupa de gradient descent si optimizare (torch.optim.SGD, ADAM, ADAMW)
tensor.squeeze() squeezes all the one dimensions (ex: 3x1x3 => 3x3)

model.parameters() returneaza parametrii modelului
model.zero_grad() reseteaza toti gradienti
loss = Calculeaza eroarea
loss.backwards() = Calculeaza noi gradienti pentru weighturi si biasuri
optimizer.step() = Updateaza weighturile si biasurile in functie de noii gradienti
https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html
https://www.youtube.com/watch?v=zduSFxRajkE
class Model(torch.nn.Module)
class Dataset(torch.utils.data.Dataset)
torch.utils.data.DataLoader(...)
torch.nn.Embedding(...) cuvant to vector de 1024 de pozitii (sau cate am eu chef)
torch.nn.ModuleList(...)
torch.nn.CrossEntropyLoss()
x.mean(1) face media matricei pe coloana
x.add_() all functions with _ do inplace operations (modifies x)
tensor.item() get value from tensor
tensor.view() changes the tensor dimension (-1=determines the right dimension size)
tensor.backward() requires argument if it is not scalar (1x1) + optimizer.zero_grad() at the end!
x.detach(), x.requires_grad_(False), with torch.no_grad()  ====> FOR TESTING 
x.grad.zero_() erases the gradients = optimizer.zero_grad()    ALWAYS DO THIS AFTER loss.backward()
requires_grad=True --> will calculate the gradient of tensor sometime
dataiter=iter(dataloader) | enumerate(dataloader)
data=data.next()
data, label=data
composed=torchvision.transforms.Compose([list of transforms])


Softmax - returns probability between 0 and 1
CrossEntropy = criterion - returns the loss for a model that returns probabilities between 0 and 1 (applies nn.LogSoftMax + nn.NLLLoss layers => no softmax in last layer) \
	n_samples * nclasses
	loss=loss_fn(Y_pred, Y), Y_pred=[[0.1, 1.0, 2.1]]) Y=[1, 0, 0]
BCELoss ---> requires sigmoid functions 
_, prediction=torch.max(Y_pred, 1) get the maxium across the first dimensions
One hot encoded: [1, 0, 0], [0, 1, 0], [0, 0, 1] - Representation for 3 possible classes (words)


Models:
https://www.youtube.com/watch?v=pDdP0TFzsoQ&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=14
Feed Forward Network - data goes from a layer to another until we have the result. - linear-relu....
Convolutional Neural Network - learns features by applying a filter pixel by pixel conv2d-relu-pool-conv2d-relu-pool-linear-linear-linear
Max Pool filter: Zooms out an image by pulling the maximum from some pixels as the maximum for one pixel (from 200x200 to 50x50)
The last layer has no activation function for CrossEntropy because it is already calling Softmax


Activation functions:
Step function
Sigmoid - Last layer for binary problems
TanH - Good for hidden layers
ReLU - THE BEST for Hidden Layers
Leaky ReLU - Solves Gradient Vanishing Problem of ReLU: if x<0 relu returns 0 and never learns because gradients are 0
Softmax - Last layer of multiclass problems
torch.nn.functional has more functions than nn





TensorBoard
from torch.utils.tensorboard import SummaryWriter
writer=SummaryWriter('folder')
img_grid=torchvision.utils.make_grid(example_data)
writer.add_image('label', img_grid)
wirter.add_graph(model, example_data.reshape(-1, 28*28))
running_loss+=loss.item() #initial 0
running_correct+=(predict==labels).sum().item() #initial 0
writer.add_scalar('training loss', running_loss/100, epoch*n_total_steps + i) #same shit for correct
labels=[]
preds=[]
labels.append(label)
###https://www.youtube.com/watch?v=VJW9wU-1n18&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=16



torch.load(file, map_device=device) #In case it was saved from another device
CPU tensors and numpy arrays they originate from share the same memory adress
https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch
https://pytorch.org/docs/stable/generated/torch.no_grad.html
https://pytorch.org/docs/stable/generated/torch.Tensor.grad.html

Models: LinearRegression, LogisticRegression
Loss: MSELoss(), BCELoss(), CrossEntropyLoss()
Optimizer: SGD() vs ADAM()
step_lr_scheduler=lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1) every 7 epochs change the learning rate by 0.1*lr
for param in model.parameters():
	param.requires_grad=False
#Freeze all layers for Transfer Learning


Vector Jacobian Product?

Tokenizer = cuvant to numar in vocabular
https://www.youtube.com/watch?v=zduSFxRajkE




MSE: Mean Squarred Error - Eroarea Patratica Medie (se foloseste patratul pentru a se evita valori negative la (y-yp)
https://www.w3schools.com/python/ref_func_zip.asp
https://www.programiz.com/python-programming/methods/built-in/zip
https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html
https://blog.bismart.com/en/classification-vs.-clustering-a-practical-explanation
https://www.geeksforgeeks.org/ml-classification-vs-clustering/



Math:
Chain Rule?



TODO: 
Make the function that reduces the MSE to the most low level. (chiar deasupra Regresie Liniara Multipla)
GRADIENT DESCENT
Metode parametrice şi metode non-parametrice?
SciKit-Learn Tutorial



Răspunsul este da. Intuitiv, o metodă mai avansată pentru a formula problema de clasificare este ca funcția  f  (sau modelul parametric) să prezică probabilitatea ca exemplu curent sa aparțină unei etichete sau alteia. O analiză mai detaliată a metodelor parametrice şi nonparametrice depăseşte scopul acestui curs. Dacă sunteţi curioşi, cu singuranţă veti găsi cursuri de aprofundare.



NLP:
Lematizare, Stemming
https://www.w3schools.com/python/python_lists_comprehension.asp
SkipGram vs CBoW
https://lightning.ai/docs/pytorch/stable/
Train vs 
Validation vs - For finetuning hyperparameters 
Test sets
**variable, *variable
loss.detach().cpu()
Adam vs AdamW
Overfitting - prea multa antrenare
PyTorch Lightning - trainer.fit()
model(batch) from dataloader vs model(sample) from test
model.eval()!



from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)
